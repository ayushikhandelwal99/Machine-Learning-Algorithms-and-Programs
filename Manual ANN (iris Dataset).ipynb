{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width           class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"iris.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width           class  \\\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa   \n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa   \n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa   \n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa   \n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa   \n",
       "..            ...          ...           ...          ...             ...   \n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica   \n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica   \n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica   \n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica   \n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica   \n",
       "\n",
       "     Iris-setosa  Iris-versicolor  Iris-virginica  \n",
       "0              1                0               0  \n",
       "1              1                0               0  \n",
       "2              1                0               0  \n",
       "3              1                0               0  \n",
       "4              1                0               0  \n",
       "..           ...              ...             ...  \n",
       "145            0                0               1  \n",
       "146            0                0               1  \n",
       "147            0                0               1  \n",
       "148            0                0               1  \n",
       "149            0                0               1  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= pd.get_dummies(df['class'])\n",
    "df = pd.concat([df,a],axis=1,sort=False)\n",
    "#df = pd.read_csv('wine.csv')\n",
    "# print(df)\n",
    "#df1=pd.read_csv('wine.csv')\n",
    "#a1 = pd.get_dummies(df1['Wine'])\n",
    "#df1 = pd.concat([df1,a1],axis=1)\n",
    "#df1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#division of data into features and labels\n",
    "x = df.drop([\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\",'class'],axis=1)\n",
    "y = df[[\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing into features and labels\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y,train_size=0.8)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(model,a0):\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 2.3916074583434557\n",
      "Accuracy after iteration 0 : 21.666666666666668 %\n",
      "Loss after iteration 100 : 0.46636499997298625\n",
      "Accuracy after iteration 100 : 78.33333333333333 %\n",
      "Loss after iteration 200 : 0.4383500775037188\n",
      "Accuracy after iteration 200 : 80.0 %\n",
      "Loss after iteration 300 : 0.34434167810681043\n",
      "Accuracy after iteration 300 : 85.0 %\n",
      "Loss after iteration 400 : 0.21798852994429976\n",
      "Accuracy after iteration 400 : 93.33333333333333 %\n",
      "Loss after iteration 500 : 0.12825844107658715\n",
      "Accuracy after iteration 500 : 96.66666666666667 %\n",
      "Loss after iteration 600 : 0.08824651240065293\n",
      "Accuracy after iteration 600 : 97.5 %\n",
      "Loss after iteration 700 : 0.05275532288115353\n",
      "Accuracy after iteration 700 : 99.16666666666667 %\n",
      "Loss after iteration 800 : 0.03544281195347693\n",
      "Accuracy after iteration 800 : 100.0 %\n",
      "Loss after iteration 900 : 0.02764177005512105\n",
      "Accuracy after iteration 900 : 100.0 %\n",
      "Loss after iteration 1000 : 0.02269024789046527\n",
      "Accuracy after iteration 1000 : 100.0 %\n",
      "Loss after iteration 1100 : 0.019199405526118332\n",
      "Accuracy after iteration 1100 : 100.0 %\n",
      "Loss after iteration 1200 : 0.016587794580576364\n",
      "Accuracy after iteration 1200 : 100.0 %\n",
      "Loss after iteration 1300 : 0.014557318692652461\n",
      "Accuracy after iteration 1300 : 100.0 %\n",
      "Loss after iteration 1400 : 0.012934822932921478\n",
      "Accuracy after iteration 1400 : 100.0 %\n",
      "Loss after iteration 1500 : 0.011610977297035207\n",
      "Accuracy after iteration 1500 : 100.0 %\n",
      "Loss after iteration 1600 : 0.010512555852501186\n",
      "Accuracy after iteration 1600 : 100.0 %\n",
      "Loss after iteration 1700 : 0.009588340207183972\n",
      "Accuracy after iteration 1700 : 100.0 %\n",
      "Loss after iteration 1800 : 0.008801314601931905\n",
      "Accuracy after iteration 1800 : 100.0 %\n",
      "Loss after iteration 1900 : 0.00812404322802522\n",
      "Accuracy after iteration 1900 : 100.0 %\n",
      "Loss after iteration 2000 : 0.007535777640439971\n",
      "Accuracy after iteration 2000 : 100.0 %\n",
      "Loss after iteration 2100 : 0.007020564447236952\n",
      "Accuracy after iteration 2100 : 100.0 %\n",
      "Loss after iteration 2200 : 0.00656596277181731\n",
      "Accuracy after iteration 2200 : 100.0 %\n",
      "Loss after iteration 2300 : 0.006162150240594497\n",
      "Accuracy after iteration 2300 : 100.0 %\n",
      "Loss after iteration 2400 : 0.00580128552197029\n",
      "Accuracy after iteration 2400 : 100.0 %\n",
      "Loss after iteration 2500 : 0.0054770451743358945\n",
      "Accuracy after iteration 2500 : 100.0 %\n",
      "Loss after iteration 2600 : 0.005184281713602355\n",
      "Accuracy after iteration 2600 : 100.0 %\n",
      "Loss after iteration 2700 : 0.004918767662261401\n",
      "Accuracy after iteration 2700 : 100.0 %\n",
      "Loss after iteration 2800 : 0.004677001657955185\n",
      "Accuracy after iteration 2800 : 100.0 %\n",
      "Loss after iteration 2900 : 0.004456060063689397\n",
      "Accuracy after iteration 2900 : 100.0 %\n",
      "Loss after iteration 3000 : 0.00425348241212537\n",
      "Accuracy after iteration 3000 : 100.0 %\n",
      "Loss after iteration 3100 : 0.004067182319168658\n",
      "Accuracy after iteration 3100 : 100.0 %\n",
      "Loss after iteration 3200 : 0.003895377768659628\n",
      "Accuracy after iteration 3200 : 100.0 %\n",
      "Loss after iteration 3300 : 0.003736536251849707\n",
      "Accuracy after iteration 3300 : 100.0 %\n",
      "Loss after iteration 3400 : 0.00358933136976789\n",
      "Accuracy after iteration 3400 : 100.0 %\n",
      "Loss after iteration 3500 : 0.0034526083214538724\n",
      "Accuracy after iteration 3500 : 100.0 %\n",
      "Loss after iteration 3600 : 0.003325356302916447\n",
      "Accuracy after iteration 3600 : 100.0 %\n",
      "Loss after iteration 3700 : 0.0032066862938366107\n",
      "Accuracy after iteration 3700 : 100.0 %\n",
      "Loss after iteration 3800 : 0.0030958130533003587\n",
      "Accuracy after iteration 3800 : 100.0 %\n",
      "Loss after iteration 3900 : 0.002992040410388828\n",
      "Accuracy after iteration 3900 : 100.0 %\n",
      "Loss after iteration 4000 : 0.0028947491398703465\n",
      "Accuracy after iteration 4000 : 100.0 %\n",
      "Loss after iteration 4100 : 0.002803386871760144\n",
      "Accuracy after iteration 4100 : 100.0 %\n",
      "Loss after iteration 4200 : 0.0027174596069550684\n",
      "Accuracy after iteration 4200 : 100.0 %\n",
      "Loss after iteration 4300 : 0.002636524507954584\n",
      "Accuracy after iteration 4300 : 100.0 %\n",
      "Loss after iteration 4400 : 0.0025601837104088842\n",
      "Accuracy after iteration 4400 : 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0c3f4fa390>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAXrElEQVR4nO3df5DU9Z3n8eeLGQYYUBEcRgISdBcUo4G4sy5ZL9GA4dTklOzGnNlclktxS21VdpPserdrtm4rt1fZlG7dxXhXu1vLaRJMbVTWTYKXGCMhGje5FTOICSgalCghjDDID51p6KZ73vdHf2ecGRoZ+tvQ9Ldfj6qp7u+3vz395lvlaz5+Pp/+fBQRmJlZtoyrdwFmZlZ7DnczswxyuJuZZZDD3cwsgxzuZmYZ1FrvAgDOO++8mDt3br3LMDNrKJs2bdoXER2VXjsjwn3u3Ll0d3fXuwwzs4Yi6ZXjveZuGTOzDHK4m5llkMPdzCyDHO5mZhnkcDczy6AThrukL0vaK2nrsHPTJK2XtD15PDc5L0n/S9KLkn4m6YpTWbyZmVU2lpb7V4HrRp27DdgQEfOADckxwPXAvORnFfD3tSnTzMxOxgnnuUfEE5Lmjjp9E3BN8nwN8Djw58n5e6O8jvCTkqZKmhkRPbUquBlEBFt/9To/enEfhwvFepdjZqfQ0gWdLLxgas1/b7VfYuocDOyI6JE0Izk/C/jlsOt2JeeOCXdJqyi37pkzZ06VZWTHkaMl/vWl1/j+tj1s2LaXV18/AoBU58LM7JSacfbEMyrcj6dSFFXcDSQiVgOrAbq6uppyx5DSQPDtn+3m4S09/Mv2feQKJdrbWrh6fgfXLujkfZfMYNrktnqXaWYNqNpw3zPY3SJpJrA3Ob8LuGDYdbOB3WkKzKotuw7xX7+1hZ/uOsTMcybyu1fMZumCGSy+aDoTx7fUuzwza3DVhvtDwArg9uRx3bDzfyTpfuC3gEPubx/p0OGj/M9HX+BrT77C9MkTuOuWRdy48G3I/S9mVkMnDHdJ91EePD1P0i7gc5RDfa2klcBO4Obk8oeBG4AXgRzwiVNQc0OKCL65+Vd84eFt7O8vsOLdc/nTZfM5e+L4epdmZhk0ltkyHz3OS0srXBvAJ9MWlTXP7X6d//Z/n+WpX+znXXOm8tVPXMlls86pd1lmlmFnxJK/WVMaCDbvPMD6bXv4/nN7eKm3n6nt47n9dy7nI10XMG6cu2DM7NRyuNdIrlDkiZ/3sv65vTz2wl729xdoHScWXzSdjy9+OzctmsW5nvliZqeJw70G9vXlWf63P2bXgcOcPbGVJZfM4NpLO3nv/A73qZtZXTjcU8oXS/zh1zaxry/P3b/fxdUXdzC+xeuxmVl9OdxTiAj+8ltb6X7lAP/7o+/i2ks7612SmRngJX9T+cqPX2Zt9y7+eMmv8+8Wvq3e5ZiZDXG4V+mJn/fy+e88x7JLO/mTa+fXuxwzsxEc7lXY0dvHH339aeZ3nsWd/36Rpzaa2RnH4X6SDh0+yn+6t5vWlnH8n9/vYvIED1uY2ZnH4X4SSgPBp+7bzM7Xcvzdx67ggmnt9S7JzKwiNztPwhfXv8APf97LFz50OYsvml7vcszMjsst9zE6crTEmv/3Ch9450x+77e8uYiZndkc7mP0o+376MsXufk3Zte7FDOzE3K4j9HDW3s4e2Irv/1r59W7FDOzE3K4j0GhOMD65/aw7B3n09bqW2ZmZz4n1Rj8+MV9vHGkyA2Xn1/vUszMxiRVuEv6tKStkp6V9Jnk3DRJ6yVtTx7PrU2p9fPwlh7OmtDKVb/uLhkzawxVh7uky4A/AK4EFgIflDQPuA3YEBHzgA3JccM6Whrg0ef2cO2lnUxo9cbVZtYY0rTcFwBPRkQuIorAD4EPATcBa5Jr1gDL05VYX//60mscOnyUGy6fWe9SzMzGLE24bwXeK2m6pHbKG2NfAHRGRA9A8jij0pslrZLULam7t7c3RRmn1ne39jC5rYX3zHOXjJk1jqrDPSK2AXcA64FHgJ8CxZN4/+qI6IqIro6OjmrLOKWKpQG+9+weli7oZOJ4d8mYWeNINaAaEfdExBUR8V5gP7Ad2CNpJkDyuDd9mfWx8Rf72d9f8CwZM2s4aWfLzEge5wC/A9wHPASsSC5ZAaxL8xn19PCWHiaNb+Hq+RV7lszMzlhpFw77Z0nTgaPAJyPigKTbgbWSVgI7gZvTFlkPpYHge8++ypIFM5jU5i4ZM2ssqcI9It5T4dxrwNI0v/dM8JOX97Ovr8ANl3mWjJk1Hn9D9Ti+u6WHiePHcc3FZ+Zgr5nZW3G4VzAwEHx366tcM3+Gd1oys4bkcK9g084D7H0jz/WeJWNmDcrhXsHDW3poax3H0gWd9S7FzKwqDvdRBgaCR7a+ytXzO5jiLhkza1AO91Ge2XWQnkNH/MUlM2toDvdRvvOzHtpa3CVjZo3N4T7Mvr489z+1k/df2snZE8fXuxwzs6o53Ie56/vbOVIc4NZl8+tdiplZKg73xEu9fXz9qZ383pVzuKhjSr3LMTNLxeGe+JtHnmdi6zg+tXRevUsxM0vN4Q50v7yf7z27hz+8+tfoOGtCvcsxM0ut6cM9IvjCw9uYcdYEVr7nwnqXY2ZWE00f7o9sfZWndx7k1mXzaW/zl5bMLBuaOtyPlga445Hnmd85hd+9Yna9yzEzq5mmDvevb9zJy6/luO36S2htaepbYWYZ07SJ9saRo9y1YTvvvmg677vY2+iZWbak3UP1TyQ9K2mrpPskTZR0oaSNkrZLekBSW62KraV/+OEO9vcX+OwNlyCp3uWYmdVU1eEuaRbwKaArIi4DWoBbgDuAOyNiHnAAWFmLQmvp1UNHuPtHO7hx4dt45+yp9S7HzKzm0nbLtAKTJLUC7UAPsAR4MHl9DbA85WfU1NZfHeIP7u1mYAD+y7+9uN7lmJmdElXP/YuIX0n6H8BO4DDwKLAJOBgRxeSyXcCsSu+XtApYBTBnzpxqyxizQ4eP8sVHX+BrT77CtMlt3HXLIi6Y1n7KP9fMrB6qDndJ5wI3ARcCB4F/Aq6vcGlUen9ErAZWA3R1dVW8phYignXP7Obz39nGa/15Pr747dy67GLOmeRVH80su9J8a+da4BcR0Qsg6RvAbwNTJbUmrffZwO70ZVZn+543+Mt1W3lyx34Wzj6Hr/zH3+Ty2efUqxwzs9MmTbjvBBZLaqfcLbMU6AYeAz4M3A+sANalLbIaP3h+D6vu3cTkCa389Ycu45bfnEPLOM+KMbPmkKbPfaOkB4GngSKwmXI3y3eA+yV9Pjl3Ty0KPVlP/Hwfba3j+MGtVzN9ihcDM7PmkmoxlYj4HPC5Uad3AFem+b210JcvMnXSeAe7mTWlzH5DNVco0j7BC4GZWXPKbLj350tMbmupdxlmZnWR4XAvMtktdzNrUtkN90LJ67ObWdPKbrjni0ye4G4ZM2tOmQ33XMHdMmbWvDIb7h5QNbNmlslwLw0Eh4+W3HI3s6aVyXDPFcqLUk72gKqZNalMhnt/vgRAuwdUzaxJZTPck5b7FHfLmFmTymS45wZb7u6WMbMmlclw78sP9rm7W8bMmlMmw31oQNXdMmbWpDIZ7v2FcreMv6FqZs0qm+Ged8vdzJpb1eEu6WJJzwz7eV3SZyRNk7Re0vbk8dxaFjwWg+HuAVUza1ZVh3tEvBARiyJiEfAbQA74JnAbsCEi5gEbkuPTanCeuwdUzaxZ1apbZinwUkS8AtwErEnOrwGW1+gzxixXKDKhdRytLZnsdTIzO6Fapd8twH3J886I6AFIHmdUeoOkVZK6JXX39vbWqIyyfq8IaWZNLnW4S2oDbgT+6WTeFxGrI6IrIro6OjrSljFCf77kmTJm1tRq0XK/Hng6IvYkx3skzQRIHvfW4DNOSn++6EXDzKyp1SLcP8qbXTIADwErkucrgHU1+IyT0l8o0u7BVDNrYqnCXVI78H7gG8NO3w68X9L25LXb03xGNcrdMm65m1nzSpWAEZEDpo869xrl2TN1kysUOf/sifUswcysrjI5V7A/X/Ja7mbW1LIZ7oWi13I3s6aWyXDP5UteesDMmlrmwr1QHKBQGmCKu2XMrIllLtwH13J3y93Mmlnmwn1oFya33M2siWUu3HNDG3W45W5mzStz4T60UYe7ZcysiWUw3N1yNzPLXrgPDai6z93Mmlf2wt37p5qZZTDchwZU3XI3s+aVuXDPeUDVzCx74T7YLTNpvFvuZta8shfuhRKT21oYN071LsXMrG4yF+65QpF2D6aaWZNLuxPTVEkPSnpe0jZJ75Y0TdJ6SduTx3NrVexY9OVLXu7XzJpe2pb7XcAjEXEJsBDYBtwGbIiIecCG5Pi0yeW9f6qZWdXhLuls4L3APQARUYiIg8BNwJrksjXA8rRFnoy+fNEzZcys6aVpuV8E9AJfkbRZ0t2SJgOdEdEDkDzOqPRmSaskdUvq7u3tTVHGSLlCyXPczazppQn3VuAK4O8j4l1APyfRBRMRqyOiKyK6Ojo6UpQxUr8HVM3MUoX7LmBXRGxMjh+kHPZ7JM0ESB73pivx5PTni0x2n7uZNbmqwz0iXgV+Keni5NRS4DngIWBFcm4FsC5VhScply95XRkza3ppU/CPgX+U1AbsAD5B+Q/GWkkrgZ3AzSk/Y8wigv6CB1TNzFKlYEQ8A3RVeGlpmt9brSNHBxgIrwhpZpapb6gOruXu2TJm1uyyFe75wY063HI3s+aWsXAvr+U+xS13M2tymQr3XMEtdzMzyFi493mLPTMzIGPhnvMWe2ZmQMbCvc9b7JmZARkL95y7ZczMgIyFe3/SLeP13M2s2WUr3PNFWsaJCa2Z+meZmZ20TKVgLtkcW/Lm2GbW3DIV7v35ovvbzczIWrgXHO5mZpC1cM+XvFGHmRmZC/eilx4wMyNr4V7wLkxmZpBysw5JLwNvACWgGBFdkqYBDwBzgZeBj0TEgXRljk2uUPTSA2Zm1Kbl/r6IWBQRgzsy3QZsiIh5wIbk+LTwbBkzs7JT0S1zE7Ameb4GWH4KPqMiD6iamZWlDfcAHpW0SdKq5FxnRPQAJI8zKr1R0ipJ3ZK6e3t7U5YBpYHg8NGSB1TNzEjZ5w5cFRG7Jc0A1kt6fqxvjIjVwGqArq6uSFnH0EYdU9wtY2aWruUeEbuTx73AN4ErgT2SZgIkj3vTFjkWg2u5t3tA1cys+nCXNFnSWYPPgWXAVuAhYEVy2QpgXdoix8JruZuZvSlNEnYC30wW6WoFvh4Rj0j6CbBW0kpgJ3Bz+jJPLJcf3IXJ4W5mVnUSRsQOYGGF868BS9MUVY3+wmDL3d0yZmaZ+YZqv3dhMjMbkp1w9+bYZmZDshPuScvd89zNzDIY7u6WMTPLULjnvDm2mdmQzIR7f75IW+s4xrdk5p9kZla1zCRhf6HopQfMzBLZCfd8yV0yZmaJDIW7W+5mZoMyE+65glvuZmaDMhPufd6FycxsSGbCPVcoekVIM7NEZsK9P1/yWu5mZonshLunQpqZDclMuOfy3j/VzGxQJsK9UBygUBrwWu5mZonU4S6pRdJmSd9Oji+UtFHSdkkPSGpLX+ZbG9wc27NlzMzKatFy/zSwbdjxHcCdETEPOACsrMFnvCWv5W5mNlKqcJc0G/gAcHdyLGAJ8GByyRpgeZrPGAuv5W5mNlLalvuXgD8DBpLj6cDBiCgmx7uAWSk/44QGw92zZczMyqoOd0kfBPZGxKbhpytcGsd5/ypJ3ZK6e3t7qy0D8FruZmajpWm5XwXcKOll4H7K3TFfAqZKGmxCzwZ2V3pzRKyOiK6I6Oro6EhRRnnpAfCAqpnZoKrDPSI+GxGzI2IucAvwg4j4GPAY8OHkshXAutRVnoBny5iZjXQq5rn/OfCnkl6k3Ad/zyn4jBH68slsGXfLmJkBUJOmbkQ8DjyePN8BXFmL3ztWOXfLmJmNkIlvqA7Oc5803i13MzPISrjni0xua2HcuEqTdczMmk8mwj1XKNLuLhkzsyGZCPe+fMmDqWZmw2Qi3HPeYs/MbIRMhHu/t9gzMxshG+HuLfbMzEbIRrgX3C1jZjZcJsI95wFVM7MRMhHu/R5QNTMboeHDPSI8oGpmNkrDh/uRowMMBB5QNTMbpuHDvb/gXZjMzEZr+HDP5Qd3YXK4m5kNavhwH9qFybNlzMyGNHy4excmM7NjNXy4v7l/qlvuZmaDqg53SRMlPSXpp5KelfRXyfkLJW2UtF3SA5LaalfusXLJRh1uuZuZvSlNyz0PLImIhcAi4DpJi4E7gDsjYh5wAFiZvszj6x/qc3e4m5kNqjrco6wvORyf/ASwBHgwOb8GWJ6qwhMYDPd2D6iamQ1J1ecuqUXSM8BeYD3wEnAwIorJJbuAWcd57ypJ3ZK6e3t7q66h390yZmbHSBXuEVGKiEXAbOBKYEGly47z3tUR0RURXR0dHVXXkCsUaRknJrQ2/NiwmVnN1CQRI+Ig8DiwGJgqabAZPRvYXYvPOJ7+ZEVIyZtjm5kNSjNbpkPS1OT5JOBaYBvwGPDh5LIVwLq0Rb4VrwhpZnasNKk4E1gjqYXyH4m1EfFtSc8B90v6PLAZuKcGdR5Xf6HowVQzs1GqDveI+Bnwrgrnd1Dufz8t+vMlLxpmZjZKw49C5gpFLxpmZjZKw4d7X77kpQfMzEZp+HDPeXNsM7NjNHy49+dL7pYxMxslA+FeZIq7ZczMRmjocC8NBIePuuVuZjZaQ4f7mxt1uOVuZjZcg4e7Fw0zM6ukocPda7mbmVXW4OHulruZWSWNHe6Dfe5eW8bMbITGDvfBXZjccjczG6Gxwz0ZUPU8dzOzkRo63HND+6e65W5mNlxDh3ufZ8uYmVXU0OE+Z1o7173jfNrdLWNmNkLVTV5JFwD3AucDA8DqiLhL0jTgAWAu8DLwkYg4kL7UYy17x/kse8f5p+JXm5k1tDQt9yJwa0QsoLwx9iclXQrcBmyIiHnAhuTYzMxOo6rDPSJ6IuLp5PkblDfHngXcBKxJLlsDLE9bpJmZnZya9LlLmkt5P9WNQGdE9ED5DwAw4zjvWSWpW1J3b29vLcowM7NE6nCXNAX4Z+AzEfH6WN8XEasjoisiujo6OtKWYWZmw6QKd0njKQf7P0bEN5LTeyTNTF6fCexNV6KZmZ2sqsNdkoB7gG0R8cVhLz0ErEierwDWVV+emZlVI823f64CPg5skfRMcu4vgNuBtZJWAjuBm9OVaGZmJ6vqcI+IHwE6zstLq/29ZmaWniKi3jUgqRd4pcq3nwfsq2E5WeB7Upnvy7F8T47VSPfk7RFRcUbKGRHuaUjqjoiuetdxJvE9qcz35Vi+J8fKyj1p6LVlzMysMoe7mVkGZSHcV9e7gDOQ70llvi/H8j05VibuScP3uZuZ2bGy0HI3M7NRHO5mZhnU0OEu6TpJL0h6UVJTrhsv6cuS9kraOuzcNEnrJW1PHs+tZ42nm6QLJD0maZukZyV9OjnftPdF0kRJT0n6aXJP/io5f6Gkjck9eUBSW71rPd0ktUjaLOnbyXEm7knDhrukFuBvgeuBS4GPJpuFNJuvAteNOtfsG6Z4I5lj5YElEbEQWARcJ2kxcAdwZ3JPDgAr61hjvXya8n4UgzJxTxo23IErgRcjYkdEFID7KW8U0lQi4glg/6jTTb1hijeSOVaU9SWH45OfAJYADybnm+qeAEiaDXwAuDs5Fhm5J40c7rOAXw473pWcszFumNIMqtlIJquS7odnKC/DvR54CTgYEcXkkmb8b+hLwJ9R3gcaYDoZuSeNHO6VFi3zvE4bUu1GMlkVEaWIWATMpvx/vgsqXXZ6q6ofSR8E9kbEpuGnK1zakPckzZK/9bYLuGDY8Wxgd51qOdPskTQzInqadcOUt9pIppnvC0BEHJT0OOXxiKmSWpOWarP9N3QVcKOkG4CJwNmUW/KZuCeN3HL/CTAvGdluA26hvFGINfmGKd5I5liSOiRNTZ5PAq6lPBbxGPDh5LKmuicR8dmImB0Rcynnxw8i4mNk5J409DdUk7+4XwJagC9HxF/XuaTTTtJ9wDWUlyndA3wO+BawFphDsmFKRIwedM0sSf8G+BdgC2/2pf4F5X73prwvkt5JeXCwhXKjbm1E/HdJF1GejDAN2Az8h4jI16/S+pB0DfCfI+KDWbknDR3uZmZWWSN3y5iZ2XE43M3MMsjhbmaWQQ53M7MMcribmWWQw93MLIMc7mZmGfT/AcB6j7hfTiXVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = initialize_parameters(nn_input_dim=4, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X_train,Y_train,learning_rate=0.09,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  93.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "test = predict(model,X_test)\n",
    "test = pd.get_dummies(test)\n",
    "Y_test = pd.DataFrame(Y_test)\n",
    "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
